#!/usr/bin/env python3
"""
===============================================================================
AIGF-NF ‚Äî Adaptive Implicit Generative Filter (Normalizing Flow)
===============================================================================

Document ID: AIGF-NF-SPEC
Version: 1.0 (Frozen, Governance-Approved)
Status: Implemented per Mathematical Specification v1.0

EXECUTIVE SUMMARY:
    AIGF-NF is a bounded, non-parametric belief model whose predictive 
    distribution evolves via constrained local adaptation and is held 
    accountable through calibration.

    AIGF-NF:
    * IS a belief model that competes in BMA
    * IS NOT a controller, signal gate, or override mechanism
    * IS governed by binary calibration pass/fail rules
    * IS bounded, auditable, and operationally constrained

DESIGN PHILOSOPHY:
    Financial returns are generated by a non-stationary stochastic process
    whose distributional shape evolves over time. Parametric tuning models
    assume fixed functional form with slowly drifting parameters.

    AIGF-NF removes the functional-form commitment and instead estimates a
    time-indexed predictive distribution:
    
        p_t(r) := p(r | z_t)
    
    where z_t ‚àà ‚Ñù^d is a latent index into a learned distribution family.

OFFLINE COMPONENT:
    A pretrained normalizing flow defines a parametric family of distributions:
    
        x = f_Œ∏(Œµ; z),  Œµ ~ N(0, I)
    
    Properties: invertibility, tractable log-density, efficient sampling.
    
    OPERATIONAL CONSTRAINT:
    - Flow parameters Œ∏ are trained offline
    - Œ∏ is frozen during live operation
    - Only the latent index z_t evolves online

ONLINE LATENT STATE EVOLUTION:
    The latent index evolves via a projected stochastic update:
    
    1. Score Signal: s_t := ‚àá_z log p(r_t | z_t)
    2. Adaptive Scaling: G_t = diag(EWMA(s_t¬≤)), Œª ‚àà [0.90, 0.99], default 0.95
    3. Gradient Clipping: |clip(x)|_‚àû ‚â§ C, default C = 5.0
    4. Update Rule: Œîz_t = Œ± ¬∑ clip(G_t^{-1/2} s_t)
    5. Projection: z_{t+1} = Œ†_ùíµ(z_t + Œîz_t)
    
    Boundedness Guarantee: sup_t |z_t| < ‚àû

PREDICTIVE DISTRIBUTION:
    At each timestep, draw samples: {r_t^(i)}_{i=1}^N ~ p(¬∑ | z_t)
    
    Sample Requirements:
    - Mean/Variance: N ‚â• 2,000
    - 1%/99% Quantiles: N ‚â• 10,000

CALIBRATION:
    PIT confidence bounds via Dvoretzky‚ÄìKiefer‚ÄìWolfowitz inequality.
    MMD with Gaussian kernel for distribution matching.

INTERPRETABILITY PROXIES (Audit Only ‚Äî DO NOT influence BMA):
    - Tail Heaviness: (q_99 - q_01) / (4œÉ)
    - Skew Direction: sign(E[(r-Œº)¬≥])
    - Novelty Score: |z_t - zÃÑ_train|_{Œ£‚Åª¬π}

NUMERICAL STABILITY MONITORING:
    |s_t|_‚àû < 10, |G_t|_‚àû < 100, clipping frequency < 5%

ARCHITECTURAL INVARIANTS:
    - No control authority
    - No regime logic
    - No governance exceptions
    - All operational permissions defined in Integration Story

===============================================================================
"""
from __future__ import annotations

import json
import math
import os
import warnings
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Callable, Dict, List, Optional, Tuple, Union

import numpy as np
from scipy import stats
from scipy.special import gammaln


# =============================================================================
# CONFIGURATION ‚Äî Specification v1.0 Parameters (FROZEN)
# =============================================================================

@dataclass(frozen=True)
class AIGFNFConfig:
    """
    AIGF-NF Configuration ‚Äî v1.0 Frozen Parameters
    
    All parameters are governance-approved and documented in the spec.
    Changes require formal review per Calibration Threshold Protocol.
    """
    
    # Latent dimension (default: moderate complexity)
    latent_dim: int = 8
    
    # EWMA decay constant for adaptive scaling (Section 4.2)
    # Default: 0.95, Permitted range: [0.90, 0.99]
    ewma_lambda: float = 0.95
    
    # Gradient clipping threshold (Section 4.3)
    # Default: 5.0 (5√ó empirical 99th percentile of score magnitudes)
    clip_threshold: float = 5.0
    
    # Step size for latent update (Section 4.4)
    # Defined in Integration Story, default conservative value
    step_size_alpha: float = 0.01
    
    # Trust region radius for projection (Section 4.5)
    # Derived from training embeddings (Mahalanobis ball)
    trust_radius: float = 3.0
    
    # Sample sizes for predictive distribution (Section 5)
    n_samples_moments: int = 2_000      # Mean/Variance estimation
    n_samples_quantiles: int = 10_000   # 1%/99% quantile estimation
    
    # PIT calibration parameters (Section 6)
    pit_epsilon: float = 0.02  # DKW confidence bound
    pit_confidence: float = 0.999  # 99.9% confidence
    
    # MMD kernel bandwidth (Section 6.2)
    # Determined via median pairwise distance heuristic
    mmd_bandwidth_multiplier: float = 1.0
    
    # Novelty alert threshold (Section 7.3)
    # 95th percentile of training distances
    novelty_alert_percentile: float = 0.95
    
    # Numerical stability bounds (Section 8)
    max_score_inf_norm: float = 10.0
    max_scaling_inf_norm: float = 100.0
    max_clipping_frequency: float = 0.05
    
    def validate(self) -> List[str]:
        """Validate configuration against governance constraints."""
        errors = []
        
        if not 0.90 <= self.ewma_lambda <= 0.99:
            errors.append(f"ewma_lambda={self.ewma_lambda} outside permitted [0.90, 0.99]")
        
        if self.clip_threshold <= 0:
            errors.append(f"clip_threshold={self.clip_threshold} must be positive")
        
        if self.step_size_alpha <= 0 or self.step_size_alpha > 1:
            errors.append(f"step_size_alpha={self.step_size_alpha} must be in (0, 1]")
        
        if self.trust_radius <= 0:
            errors.append(f"trust_radius={self.trust_radius} must be positive")
        
        if self.n_samples_moments < 2000:
            errors.append(f"n_samples_moments={self.n_samples_moments} below minimum 2000")
        
        if self.n_samples_quantiles < 10000:
            errors.append(f"n_samples_quantiles={self.n_samples_quantiles} below minimum 10000")
        
        return errors
    
    def to_dict(self) -> Dict[str, Any]:
        """Export configuration to dictionary."""
        return {
            'latent_dim': self.latent_dim,
            'ewma_lambda': self.ewma_lambda,
            'clip_threshold': self.clip_threshold,
            'step_size_alpha': self.step_size_alpha,
            'trust_radius': self.trust_radius,
            'n_samples_moments': self.n_samples_moments,
            'n_samples_quantiles': self.n_samples_quantiles,
            'pit_epsilon': self.pit_epsilon,
            'pit_confidence': self.pit_confidence,
            'mmd_bandwidth_multiplier': self.mmd_bandwidth_multiplier,
            'novelty_alert_percentile': self.novelty_alert_percentile,
            'max_score_inf_norm': self.max_score_inf_norm,
            'max_scaling_inf_norm': self.max_scaling_inf_norm,
            'max_clipping_frequency': self.max_clipping_frequency,
        }
    
    @classmethod
    def from_dict(cls, d: Dict[str, Any]) -> 'AIGFNFConfig':
        """Load configuration from dictionary."""
        return cls(**{k: v for k, v in d.items() if k in cls.__dataclass_fields__})


# Default configuration (governance-approved)
DEFAULT_AIGF_NF_CONFIG = AIGFNFConfig()


# =============================================================================
# FLOW ARTIFACT ‚Äî Pretrained Normalizing Flow Representation
# =============================================================================

@dataclass
class FlowArtifact:
    """
    Pretrained normalizing flow artifact.
    
    This represents the offline-trained component that maps latent codes
    to distributions. During live operation, only z_t evolves; flow 
    parameters Œ∏ remain frozen.
    
    The flow is represented as a simple affine RealNVP for tractability,
    but the interface supports swapping in more sophisticated architectures.
    """
    
    # Artifact metadata
    artifact_id: str
    version: str
    training_date: str
    
    # Architecture specification
    latent_dim: int
    n_coupling_layers: int
    hidden_dim: int
    
    # Learned parameters (FROZEN)
    # For RealNVP: alternating scale/shift networks
    coupling_weights: List[np.ndarray] = field(default_factory=list)
    coupling_biases: List[np.ndarray] = field(default_factory=list)
    
    # Training distribution statistics (for trust region)
    z_train_mean: np.ndarray = field(default_factory=lambda: np.zeros(8))
    z_train_cov: np.ndarray = field(default_factory=lambda: np.eye(8))
    z_train_cov_inv: np.ndarray = field(default_factory=lambda: np.eye(8))
    
    # Empirical score magnitude statistics (for clipping calibration)
    score_percentile_99: float = 1.0
    
    @classmethod
    def create_default(cls, latent_dim: int = 8) -> 'FlowArtifact':
        """
        Create a default flow artifact for initialization.
        
        This creates a simple identity-like flow that can be used before
        actual training data is available. In production, this should be
        replaced with a properly trained artifact.
        """
        return cls(
            artifact_id='default_init',
            version='0.0.1',
            training_date='2026-02-01',
            latent_dim=latent_dim,
            n_coupling_layers=4,
            hidden_dim=32,
            coupling_weights=[],
            coupling_biases=[],
            z_train_mean=np.zeros(latent_dim),
            z_train_cov=np.eye(latent_dim),
            z_train_cov_inv=np.eye(latent_dim),
            score_percentile_99=1.0,
        )
    
    def to_dict(self) -> Dict[str, Any]:
        """Export artifact to dictionary for persistence."""
        return {
            'artifact_id': self.artifact_id,
            'version': self.version,
            'training_date': self.training_date,
            'latent_dim': self.latent_dim,
            'n_coupling_layers': self.n_coupling_layers,
            'hidden_dim': self.hidden_dim,
            'coupling_weights': [w.tolist() for w in self.coupling_weights],
            'coupling_biases': [b.tolist() for b in self.coupling_biases],
            'z_train_mean': self.z_train_mean.tolist(),
            'z_train_cov': self.z_train_cov.tolist(),
            'z_train_cov_inv': self.z_train_cov_inv.tolist(),
            'score_percentile_99': self.score_percentile_99,
        }
    
    @classmethod
    def from_dict(cls, d: Dict[str, Any]) -> 'FlowArtifact':
        """Load artifact from dictionary."""
        return cls(
            artifact_id=d['artifact_id'],
            version=d['version'],
            training_date=d['training_date'],
            latent_dim=d['latent_dim'],
            n_coupling_layers=d['n_coupling_layers'],
            hidden_dim=d['hidden_dim'],
            coupling_weights=[np.array(w) for w in d.get('coupling_weights', [])],
            coupling_biases=[np.array(b) for b in d.get('coupling_biases', [])],
            z_train_mean=np.array(d['z_train_mean']),
            z_train_cov=np.array(d['z_train_cov']),
            z_train_cov_inv=np.array(d['z_train_cov_inv']),
            score_percentile_99=d.get('score_percentile_99', 1.0),
        )


# =============================================================================
# LATENT STATE ‚Äî Online Adaptive Component
# =============================================================================

@dataclass
class LatentState:
    """
    Online latent state that evolves via projected stochastic updates.
    
    This is the ONLY component that changes during live operation.
    All flow parameters remain frozen.
    """
    
    # Current latent position
    z: np.ndarray
    
    # EWMA of squared scores for adaptive scaling (Section 4.2)
    ewma_score_sq: np.ndarray
    
    # Update counter for diagnostics
    n_updates: int = 0
    
    # Clipping statistics for monitoring (Section 8)
    n_clipped: int = 0
    
    # Last update magnitude for diagnostics
    last_update_norm: float = 0.0
    
    # Projection rejection counter
    n_projection_rejections: int = 0
    
    @classmethod
    def initialize(cls, latent_dim: int, rng: Optional[np.random.Generator] = None) -> 'LatentState':
        """Initialize latent state at origin (prior mean)."""
        if rng is None:
            rng = np.random.default_rng()
        
        return cls(
            z=np.zeros(latent_dim),
            ewma_score_sq=np.ones(latent_dim) * 0.01,  # Small initial variance
            n_updates=0,
            n_clipped=0,
            last_update_norm=0.0,
            n_projection_rejections=0,
        )
    
    @property
    def clipping_frequency(self) -> float:
        """Fraction of updates that required clipping."""
        if self.n_updates == 0:
            return 0.0
        return self.n_clipped / self.n_updates
    
    @property
    def projection_rejection_rate(self) -> float:
        """Fraction of updates rejected due to projection failure."""
        if self.n_updates == 0:
            return 0.0
        return self.n_projection_rejections / self.n_updates
    
    def to_dict(self) -> Dict[str, Any]:
        """Export state to dictionary."""
        return {
            'z': self.z.tolist(),
            'ewma_score_sq': self.ewma_score_sq.tolist(),
            'n_updates': self.n_updates,
            'n_clipped': self.n_clipped,
            'last_update_norm': self.last_update_norm,
            'n_projection_rejections': self.n_projection_rejections,
        }
    
    @classmethod
    def from_dict(cls, d: Dict[str, Any]) -> 'LatentState':
        """Load state from dictionary."""
        return cls(
            z=np.array(d['z']),
            ewma_score_sq=np.array(d['ewma_score_sq']),
            n_updates=d.get('n_updates', 0),
            n_clipped=d.get('n_clipped', 0),
            last_update_norm=d.get('last_update_norm', 0.0),
            n_projection_rejections=d.get('n_projection_rejections', 0),
        )


# =============================================================================
# INTERPRETABILITY PROXIES ‚Äî Audit-Only Outputs (Section 7)
# =============================================================================

@dataclass
class InterpretabilityProxies:
    """
    Interpretability proxies for audit purposes.
    
    CRITICAL: These outputs DO NOT influence BMA weights.
    They are for human understanding and model review only.
    """
    
    # Tail heaviness proxy (Section 7.1)
    # ‚âà1 ‚Üí Gaussian-like, >1 ‚Üí heavy-tailed, <1 ‚Üí light-tailed
    tail_heaviness: float
    
    # Skew direction (Section 7.2)
    # +1 for right-skewed, -1 for left-skewed, 0 for symmetric
    skew_sign: int
    
    # Novelty score (Section 7.3)
    # Mahalanobis distance from training center
    novelty_score: float
    
    # Alert status
    novelty_alert: bool = False
    
    def to_dict(self) -> Dict[str, Any]:
        """Export proxies to dictionary."""
        return {
            'tail_heaviness': float(self.tail_heaviness),
            'skew_sign': int(self.skew_sign),
            'novelty_score': float(self.novelty_score),
            'novelty_alert': bool(self.novelty_alert),
        }


# =============================================================================
# NUMERICAL STABILITY MONITORING (Section 8)
# =============================================================================

@dataclass
class StabilityDiagnostics:
    """
    Numerical stability diagnostics per Section 8.
    
    Persistent violations indicate flow inadequacy and trigger review.
    """
    
    # Score infinity norm (expected < 10)
    score_inf_norm: float
    
    # Scaling matrix infinity norm (expected < 100)
    scaling_inf_norm: float
    
    # Update magnitude (expected < trust_radius)
    update_norm: float
    
    # Rolling clipping frequency
    clipping_frequency: float
    
    # Violation flags
    score_violation: bool = False
    scaling_violation: bool = False
    update_violation: bool = False
    clipping_violation: bool = False
    
    @property
    def any_violation(self) -> bool:
        """True if any stability violation detected."""
        return (
            self.score_violation or 
            self.scaling_violation or 
            self.update_violation or 
            self.clipping_violation
        )
    
    def to_dict(self) -> Dict[str, Any]:
        """Export diagnostics to dictionary."""
        return {
            'score_inf_norm': float(self.score_inf_norm),
            'scaling_inf_norm': float(self.scaling_inf_norm),
            'update_norm': float(self.update_norm),
            'clipping_frequency': float(self.clipping_frequency),
            'score_violation': bool(self.score_violation),
            'scaling_violation': bool(self.scaling_violation),
            'update_violation': bool(self.update_violation),
            'clipping_violation': bool(self.clipping_violation),
            'any_violation': bool(self.any_violation),
        }


# =============================================================================
# CALIBRATION RESULT (Section 6)
# =============================================================================

@dataclass
class AIGFNFCalibrationResult:
    """
    Calibration diagnostics result per Section 6.
    
    PIT confidence bounds use Dvoretzky‚ÄìKiefer‚ÄìWolfowitz inequality.
    MMD uses Gaussian kernel with median bandwidth.
    """
    
    # PIT uniformity test (KS test)
    pit_ks_statistic: float
    pit_ks_pvalue: float
    pit_passed: bool
    
    # MMD statistic
    mmd_statistic: float
    mmd_threshold: float
    mmd_passed: bool
    
    # Overall calibration status
    calibrated: bool
    
    # Sample size used
    n_samples: int
    
    def to_dict(self) -> Dict[str, Any]:
        """Export calibration result to dictionary."""
        return {
            'pit_ks_statistic': float(self.pit_ks_statistic),
            'pit_ks_pvalue': float(self.pit_ks_pvalue),
            'pit_passed': bool(self.pit_passed),
            'mmd_statistic': float(self.mmd_statistic),
            'mmd_threshold': float(self.mmd_threshold),
            'mmd_passed': bool(self.mmd_passed),
            'calibrated': bool(self.calibrated),
            'n_samples': int(self.n_samples),
        }


# =============================================================================
# PREDICTIVE RESULT ‚Äî Output of AIGF-NF Inference
# =============================================================================

@dataclass
class AIGFNFPredictiveResult:
    """
    Predictive distribution result from AIGF-NF.
    
    Contains:
    - Samples from predictive distribution
    - Estimated moments
    - Calibration diagnostics
    - Interpretability proxies
    - Stability diagnostics
    """
    
    # Predictive samples
    samples: np.ndarray
    
    # Estimated moments (Section 5)
    mean: float
    variance: float
    std: float
    
    # Tail quantiles
    quantile_01: float  # 1% quantile
    quantile_99: float  # 99% quantile
    
    # Log-likelihood at current observation (if available)
    log_likelihood: Optional[float] = None
    
    # Calibration result (optional, computed on demand)
    calibration: Optional[AIGFNFCalibrationResult] = None
    
    # Interpretability proxies (audit only)
    proxies: Optional[InterpretabilityProxies] = None
    
    # Stability diagnostics
    stability: Optional[StabilityDiagnostics] = None
    
    # Latent state snapshot
    latent_z: Optional[np.ndarray] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """Export result to dictionary."""
        result = {
            'samples': self.samples.tolist() if len(self.samples) <= 1000 else self.samples[:1000].tolist(),
            'n_samples': len(self.samples),
            'mean': float(self.mean),
            'variance': float(self.variance),
            'std': float(self.std),
            'quantile_01': float(self.quantile_01),
            'quantile_99': float(self.quantile_99),
            'log_likelihood': float(self.log_likelihood) if self.log_likelihood is not None else None,
        }
        
        if self.calibration is not None:
            result['calibration'] = self.calibration.to_dict()
        
        if self.proxies is not None:
            result['proxies'] = self.proxies.to_dict()
        
        if self.stability is not None:
            result['stability'] = self.stability.to_dict()
        
        if self.latent_z is not None:
            result['latent_z'] = self.latent_z.tolist()
        
        return result


# =============================================================================
# AIGF-NF MODEL ‚Äî Core Implementation
# =============================================================================

class AIGFNFModel:
    """
    Adaptive Implicit Generative Filter (Normalizing Flow) ‚Äî v1.0 Implementation
    
    This class implements the AIGF-NF specification as a belief model that 
    competes in Bayesian Model Averaging.
    
    ARCHITECTURE:
        - Offline: Pretrained normalizing flow (frozen Œ∏)
        - Online: Adaptive latent state z_t via projected updates
    
    INTEGRATION:
        - tune.py: Fits via filter() and optimize_params()
        - signals.py: Samples via sample_predictive()
        - BMA: Competes with Student-t, Gaussian, etc. via log_likelihood
    
    GOVERNANCE:
        - No control authority
        - No regime logic  
        - No governance exceptions
        - Binary calibration pass/fail rules
    """
    
    def __init__(
        self,
        config: Optional[AIGFNFConfig] = None,
        flow_artifact: Optional[FlowArtifact] = None,
        latent_state: Optional[LatentState] = None,
        rng: Optional[np.random.Generator] = None,
    ):
        """
        Initialize AIGF-NF model.
        
        Args:
            config: Model configuration (uses default if None)
            flow_artifact: Pretrained flow (creates default if None)
            latent_state: Current latent state (initializes at origin if None)
            rng: Random number generator
        """
        self.config = config or DEFAULT_AIGF_NF_CONFIG
        
        # Validate configuration
        errors = self.config.validate()
        if errors:
            raise ValueError(f"Invalid AIGF-NF configuration: {errors}")
        
        # Initialize or load flow artifact
        if flow_artifact is None:
            self.flow_artifact = FlowArtifact.create_default(self.config.latent_dim)
        else:
            self.flow_artifact = flow_artifact
            if flow_artifact.latent_dim != self.config.latent_dim:
                raise ValueError(
                    f"Flow artifact latent_dim={flow_artifact.latent_dim} "
                    f"doesn't match config latent_dim={self.config.latent_dim}"
                )
        
        # Initialize random number generator
        self.rng = rng if rng is not None else np.random.default_rng()
        
        # Initialize latent state
        if latent_state is None:
            self.state = LatentState.initialize(self.config.latent_dim, self.rng)
        else:
            self.state = latent_state
        
        # Precompute trust region bounds
        self._trust_region_center = self.flow_artifact.z_train_mean
        self._trust_region_cov_inv = self.flow_artifact.z_train_cov_inv
    
    # =========================================================================
    # SECTION 3: DISTRIBUTION FAMILY (Flow-Based Sampling)
    # =========================================================================
    
    def _flow_forward(self, z: np.ndarray, epsilon: np.ndarray) -> np.ndarray:
        """
        Forward pass through normalizing flow: z ‚Üí x.
        
        For a simple affine flow (RealNVP-like):
            x = f_Œ∏(Œµ; z) where Œµ ~ N(0, I)
        
        This is a simplified implementation. Production should use a proper
        deep flow architecture (RealNVP, MAF, NSF, etc.).
        
        Args:
            z: Latent conditioning vector (d-dimensional)
            epsilon: Base distribution samples (N x d)
            
        Returns:
            Transformed samples (N x 1 for return distribution)
        """
        # Simple location-scale flow conditioned on z
        # Production: Replace with deep coupling layers
        
        # Extract location and scale from z (first two components)
        mu = z[0] if len(z) > 0 else 0.0
        log_sigma = z[1] if len(z) > 1 else 0.0
        sigma = np.exp(np.clip(log_sigma, -5, 5))  # Numerical stability
        
        # Tail adjustment from remaining components (mixture of Gaussian bases)
        tail_weight = 0.0
        if len(z) > 2:
            # z[2:] encode mixture weights for tail adjustment
            tail_components = z[2:]
            # Sigmoid to get mixture weight
            tail_weight = 1.0 / (1.0 + np.exp(-np.mean(tail_components)))
        
        # Generate samples with tail adjustment
        # Main component: Gaussian
        # Tail component: Student-t with ŒΩ=4 for heavy tails
        gaussian_samples = epsilon[:, 0] if epsilon.ndim > 1 else epsilon
        
        if tail_weight > 0.01:
            # Mix in Student-t samples for tail heaviness
            n_samples = len(gaussian_samples)
            t_samples = self.rng.standard_t(df=4.0, size=n_samples)
            mix_mask = self.rng.random(n_samples) < tail_weight
            samples = np.where(mix_mask, t_samples, gaussian_samples)
        else:
            samples = gaussian_samples
        
        # Apply location-scale transform
        return mu + sigma * samples
    
    def _flow_log_density(self, x: float, z: np.ndarray) -> float:
        """
        Compute log density at x given latent z.
        
        For the simplified flow:
            log p(x | z) = log N(x; Œº(z), œÉ¬≤(z)) + log det Jacobian
        
        Production: Replace with proper flow density computation.
        
        Args:
            x: Observation (return)
            z: Latent conditioning vector
            
        Returns:
            Log density value
        """
        mu = z[0] if len(z) > 0 else 0.0
        log_sigma = z[1] if len(z) > 1 else 0.0
        sigma = np.exp(np.clip(log_sigma, -5, 5))
        
        # Tail adjustment
        tail_weight = 0.0
        if len(z) > 2:
            tail_components = z[2:]
            tail_weight = 1.0 / (1.0 + np.exp(-np.mean(tail_components)))
        
        # Mixture log density
        z_score = (x - mu) / sigma
        
        # Gaussian component
        log_p_gaussian = -0.5 * z_score**2 - 0.5 * np.log(2 * np.pi) - log_sigma
        
        if tail_weight > 0.01:
            # Student-t component (ŒΩ=4)
            nu = 4.0
            log_p_t = (
                gammaln((nu + 1) / 2) - gammaln(nu / 2)
                - 0.5 * np.log(nu * np.pi)
                - log_sigma
                - ((nu + 1) / 2) * np.log(1 + z_score**2 / nu)
            )
            # Log mixture: log(w1*p1 + w2*p2) ‚âà max + log(exp(...) + exp(...))
            log_p = np.logaddexp(
                np.log(1 - tail_weight) + log_p_gaussian,
                np.log(tail_weight) + log_p_t
            )
        else:
            log_p = log_p_gaussian
        
        return float(log_p)
    
    def _compute_score(self, x: float, z: np.ndarray) -> np.ndarray:
        """
        Compute score: s_t = ‚àá_z log p(x | z)
        
        Uses numerical differentiation for robustness.
        Production: Use autograd for exact gradients.
        
        Args:
            x: Observation
            z: Current latent state
            
        Returns:
            Score vector (same dimension as z)
        """
        eps = 1e-5
        score = np.zeros_like(z)
        
        log_p_current = self._flow_log_density(x, z)
        
        for i in range(len(z)):
            z_plus = z.copy()
            z_plus[i] += eps
            log_p_plus = self._flow_log_density(x, z_plus)
            score[i] = (log_p_plus - log_p_current) / eps
        
        return score
    
    # =========================================================================
    # SECTION 4: ONLINE LATENT STATE EVOLUTION
    # =========================================================================
    
    def _clip_gradient(self, g: np.ndarray) -> Tuple[np.ndarray, bool]:
        """
        Apply gradient clipping per Section 4.3.
        
        |clip(x)|_‚àû ‚â§ C where C = 5.0 (default)
        
        Args:
            g: Gradient vector
            
        Returns:
            Tuple of (clipped gradient, was_clipped flag)
        """
        C = self.config.clip_threshold
        inf_norm = np.max(np.abs(g))
        
        if inf_norm > C:
            clipped = g * (C / inf_norm)
            return clipped, True
        else:
            return g, False
    
    def _project_to_trust_region(self, z: np.ndarray) -> Tuple[np.ndarray, bool]:
        """
        Project z onto trust region ùíµ per Section 4.5.
        
        Trust region is a Mahalanobis ball centered at training mean:
            ùíµ = {z : (z - Œº)·µÄ Œ£‚Åª¬π (z - Œº) ‚â§ R¬≤}
        
        Args:
            z: Latent vector to project
            
        Returns:
            Tuple of (projected z, was_projected flag)
        """
        delta = z - self._trust_region_center
        
        # Mahalanobis distance squared
        mahal_sq = np.dot(delta, np.dot(self._trust_region_cov_inv, delta))
        mahal_dist = np.sqrt(max(mahal_sq, 0))
        
        R = self.config.trust_radius
        
        if mahal_dist > R:
            # Project to boundary: z_proj = center + (R/||delta||_M) * delta
            scale = R / mahal_dist
            z_proj = self._trust_region_center + scale * delta
            return z_proj, True
        else:
            return z, False
    
    def update(self, r_t: float) -> StabilityDiagnostics:
        """
        Perform one-step latent state update given observed return.
        
        Implements the full update rule from Section 4:
            1. Score: s_t = ‚àá_z log p(r_t | z_t)
            2. EWMA scaling: G_t = diag(EWMA(s_t¬≤))
            3. Clipping: clip(G_t^{-1/2} s_t)
            4. Update: Œîz_t = Œ± ¬∑ clip(...)
            5. Projection: z_{t+1} = Œ†_ùíµ(z_t + Œîz_t)
        
        Args:
            r_t: Observed return
            
        Returns:
            Stability diagnostics for this update
        """
        # Step 1: Compute score
        s_t = self._compute_score(r_t, self.state.z)
        score_inf_norm = float(np.max(np.abs(s_t)))
        
        # Step 2: Update EWMA of squared scores
        lam = self.config.ewma_lambda
        self.state.ewma_score_sq = lam * self.state.ewma_score_sq + (1 - lam) * s_t**2
        
        # Compute adaptive scaling matrix G_t^{-1/2}
        G_inv_sqrt = 1.0 / np.sqrt(self.state.ewma_score_sq + 1e-8)
        scaling_inf_norm = float(np.max(np.abs(1.0 / G_inv_sqrt)))
        
        # Step 3: Scale and clip gradient
        scaled_grad = G_inv_sqrt * s_t
        clipped_grad, was_clipped = self._clip_gradient(scaled_grad)
        
        if was_clipped:
            self.state.n_clipped += 1
        
        # Step 4: Compute update
        alpha = self.config.step_size_alpha
        delta_z = alpha * clipped_grad
        update_norm = float(np.linalg.norm(delta_z))
        
        # Step 5: Apply update and project
        z_candidate = self.state.z + delta_z
        z_projected, was_projected = self._project_to_trust_region(z_candidate)
        
        if was_projected:
            # Check if projection moved too far (rejection condition)
            projection_distance = np.linalg.norm(z_projected - z_candidate)
            if projection_distance > self.config.trust_radius * 0.5:
                # Reject update, keep current state
                self.state.n_projection_rejections += 1
            else:
                self.state.z = z_projected
        else:
            self.state.z = z_projected
        
        # Update counters
        self.state.n_updates += 1
        self.state.last_update_norm = update_norm
        
        # Build stability diagnostics
        diagnostics = StabilityDiagnostics(
            score_inf_norm=score_inf_norm,
            scaling_inf_norm=scaling_inf_norm,
            update_norm=update_norm,
            clipping_frequency=self.state.clipping_frequency,
            score_violation=score_inf_norm > self.config.max_score_inf_norm,
            scaling_violation=scaling_inf_norm > self.config.max_scaling_inf_norm,
            update_violation=update_norm > self.config.trust_radius,
            clipping_violation=self.state.clipping_frequency > self.config.max_clipping_frequency,
        )
        
        return diagnostics
    
    # =========================================================================
    # SECTION 5: PREDICTIVE DISTRIBUTION AND MOMENTS
    # =========================================================================
    
    def sample_predictive(self, n_samples: Optional[int] = None) -> np.ndarray:
        """
        Draw samples from predictive distribution p(¬∑ | z_t).
        
        Args:
            n_samples: Number of samples (default: n_samples_quantiles from config)
            
        Returns:
            Array of samples
        """
        if n_samples is None:
            n_samples = self.config.n_samples_quantiles
        
        # Generate base distribution samples
        epsilon = self.rng.standard_normal((n_samples, self.config.latent_dim))
        
        # Transform through flow
        samples = self._flow_forward(self.state.z, epsilon)
        
        return samples
    
    def compute_predictive(
        self,
        compute_calibration: bool = False,
        observed_returns: Optional[np.ndarray] = None,
    ) -> AIGFNFPredictiveResult:
        """
        Compute full predictive result with moments and diagnostics.
        
        Args:
            compute_calibration: Whether to compute calibration diagnostics
            observed_returns: Historical returns for calibration (required if compute_calibration=True)
            
        Returns:
            Complete predictive result
        """
        # Draw samples for quantile estimation
        samples = self.sample_predictive(self.config.n_samples_quantiles)
        
        # Compute moments (Section 5)
        mean = float(np.mean(samples))
        variance = float(np.var(samples))
        std = float(np.std(samples))
        
        # Compute quantiles
        quantile_01 = float(np.percentile(samples, 1))
        quantile_99 = float(np.percentile(samples, 99))
        
        # Compute interpretability proxies (Section 7)
        proxies = self._compute_interpretability_proxies(samples)
        
        # Compute stability diagnostics
        stability = StabilityDiagnostics(
            score_inf_norm=0.0,  # Will be updated on next observation
            scaling_inf_norm=float(np.max(1.0 / np.sqrt(self.state.ewma_score_sq + 1e-8))),
            update_norm=self.state.last_update_norm,
            clipping_frequency=self.state.clipping_frequency,
        )
        
        # Compute calibration if requested
        calibration = None
        if compute_calibration and observed_returns is not None:
            calibration = self._compute_calibration(observed_returns)
        
        return AIGFNFPredictiveResult(
            samples=samples,
            mean=mean,
            variance=variance,
            std=std,
            quantile_01=quantile_01,
            quantile_99=quantile_99,
            calibration=calibration,
            proxies=proxies,
            stability=stability,
            latent_z=self.state.z.copy(),
        )
    
    def _compute_interpretability_proxies(self, samples: np.ndarray) -> InterpretabilityProxies:
        """
        Compute interpretability proxies per Section 7.
        
        These are for AUDIT ONLY and DO NOT influence BMA.
        """
        # Section 7.1: Tail Heaviness
        q01 = float(np.percentile(samples, 1))
        q99 = float(np.percentile(samples, 99))
        sigma = float(np.std(samples))
        
        if sigma > 1e-10:
            tail_heaviness = (q99 - q01) / (4 * sigma)
        else:
            tail_heaviness = 1.0
        
        # Section 7.2: Skew Direction
        mu = float(np.mean(samples))
        third_moment = float(np.mean((samples - mu) ** 3))
        skew_sign = int(np.sign(third_moment))
        
        # Section 7.3: Novelty Score
        delta = self.state.z - self._trust_region_center
        novelty_score = float(np.sqrt(
            np.dot(delta, np.dot(self._trust_region_cov_inv, delta))
        ))
        
        # Alert if novelty exceeds threshold
        novelty_alert = novelty_score > self.config.trust_radius * self.config.novelty_alert_percentile
        
        return InterpretabilityProxies(
            tail_heaviness=tail_heaviness,
            skew_sign=skew_sign,
            novelty_score=novelty_score,
            novelty_alert=novelty_alert,
        )
    
    # =========================================================================
    # SECTION 6: CALIBRATION
    # =========================================================================
    
    def _compute_calibration(self, observed_returns: np.ndarray) -> AIGFNFCalibrationResult:
        """
        Compute calibration diagnostics per Section 6.
        
        Uses PIT uniformity test and MMD statistic.
        """
        n_obs = len(observed_returns)
        
        # Compute PIT values
        pit_values = np.zeros(n_obs)
        for i, r in enumerate(observed_returns):
            # Empirical CDF via sampling
            samples = self.sample_predictive(self.config.n_samples_moments)
            pit_values[i] = np.mean(samples <= r)
        
        # Clip PIT values for numerical stability
        pit_values = np.clip(pit_values, self.config.pit_epsilon, 1 - self.config.pit_epsilon)
        
        # KS test against Uniform(0,1)
        ks_stat, ks_pvalue = stats.kstest(pit_values, 'uniform')
        
        # DKW confidence bound (Section 6.1)
        epsilon = self.config.pit_epsilon
        dkw_threshold = np.sqrt(-0.5 * np.log(0.5 * (1 - self.config.pit_confidence)) / n_obs)
        pit_passed = ks_stat <= dkw_threshold
        
        # Compute MMD (Section 6.2)
        mmd_stat, mmd_threshold = self._compute_mmd(observed_returns)
        mmd_passed = mmd_stat <= mmd_threshold
        
        # Overall calibration status
        calibrated = pit_passed and mmd_passed
        
        return AIGFNFCalibrationResult(
            pit_ks_statistic=float(ks_stat),
            pit_ks_pvalue=float(ks_pvalue),
            pit_passed=pit_passed,
            mmd_statistic=mmd_stat,
            mmd_threshold=mmd_threshold,
            mmd_passed=mmd_passed,
            calibrated=calibrated,
            n_samples=n_obs,
        )
    
    def _compute_mmd(self, observed_returns: np.ndarray) -> Tuple[float, float]:
        """
        Compute Maximum Mean Discrepancy with Gaussian kernel.
        
        Kernel: k(x,y) = exp(-|x-y|¬≤ / 2œÉ¬≤)
        Bandwidth: median pairwise distance heuristic
        """
        n = len(observed_returns)
        
        # Generate model samples
        model_samples = self.sample_predictive(n)
        
        # Median pairwise distance for bandwidth
        all_data = np.concatenate([observed_returns, model_samples])
        pairwise_dists = np.abs(all_data[:, None] - all_data[None, :])
        sigma = float(np.median(pairwise_dists[pairwise_dists > 0]))
        sigma *= self.config.mmd_bandwidth_multiplier
        
        if sigma < 1e-10:
            sigma = 1.0
        
        def kernel(x, y):
            return np.exp(-0.5 * (x - y)**2 / sigma**2)
        
        # MMD¬≤ = E[k(X,X')] - 2E[k(X,Y)] + E[k(Y,Y')]
        # Unbiased estimator
        
        # k(X, X')
        kxx = 0.0
        for i in range(n):
            for j in range(i + 1, n):
                kxx += kernel(observed_returns[i], observed_returns[j])
        kxx *= 2 / (n * (n - 1))
        
        # k(Y, Y')
        kyy = 0.0
        for i in range(n):
            for j in range(i + 1, n):
                kyy += kernel(model_samples[i], model_samples[j])
        kyy *= 2 / (n * (n - 1))
        
        # k(X, Y)
        kxy = 0.0
        for i in range(n):
            for j in range(n):
                kxy += kernel(observed_returns[i], model_samples[j])
        kxy /= (n * n)
        
        mmd_sq = max(kxx - 2 * kxy + kyy, 0)
        mmd_stat = float(np.sqrt(mmd_sq))
        
        # Threshold: calibrated offline per asset (simplified: use 2œÉ bound)
        # In production, this should be loaded from governance artifacts
        mmd_threshold = 2.0 / np.sqrt(n)
        
        return mmd_stat, mmd_threshold
    
    # =========================================================================
    # BMA INTERFACE ‚Äî Compatibility with tune.py and signals.py
    # =========================================================================
    
    def log_likelihood(self, returns: np.ndarray) -> float:
        """
        Compute total log-likelihood for BMA weight computation.
        
        Args:
            returns: Array of observed returns
            
        Returns:
            Total log-likelihood
        """
        total_ll = 0.0
        for r in returns:
            total_ll += self._flow_log_density(r, self.state.z)
        return total_ll
    
    def mean_log_likelihood(self, returns: np.ndarray) -> float:
        """
        Compute mean log-likelihood for BIC computation.
        """
        n = len(returns)
        if n == 0:
            return -np.inf
        return self.log_likelihood(returns) / n
    
    def filter(self, returns: np.ndarray) -> Dict[str, Any]:
        """
        Run filter over return series, updating latent state.
        
        This is the main entry point for tune.py integration.
        
        Args:
            returns: Array of observed returns
            
        Returns:
            Dictionary with filter results compatible with tune.py
        """
        # Initialize fresh state
        self.state = LatentState.initialize(self.config.latent_dim, self.rng)
        
        log_likelihoods = []
        stability_violations = 0
        
        for r in returns:
            # Record log-likelihood before update
            ll = self._flow_log_density(r, self.state.z)
            log_likelihoods.append(ll)
            
            # Update latent state
            diagnostics = self.update(r)
            if diagnostics.any_violation:
                stability_violations += 1
        
        total_ll = sum(log_likelihoods)
        mean_ll = total_ll / len(returns) if returns.size > 0 else -np.inf
        
        # Compute final predictive
        predictive = self.compute_predictive()
        
        return {
            'mean_log_likelihood': mean_ll,
            'total_log_likelihood': total_ll,
            'n_observations': len(returns),
            'latent_z': self.state.z.tolist(),
            'latent_dim': self.config.latent_dim,
            'n_updates': self.state.n_updates,
            'clipping_frequency': self.state.clipping_frequency,
            'stability_violations': stability_violations,
            'predictive_mean': predictive.mean,
            'predictive_std': predictive.std,
            'tail_heaviness': predictive.proxies.tail_heaviness if predictive.proxies else None,
            'novelty_score': predictive.proxies.novelty_score if predictive.proxies else None,
        }
    
    def optimize_params(
        self,
        returns: np.ndarray,
        prior_mean: float = 0.0,
        prior_lambda: float = 0.1,
    ) -> Dict[str, Any]:
        """
        Optimize AIGF-NF parameters via filtering.
        
        For AIGF-NF, "optimization" means running the filter to learn
        the latent state trajectory. The flow parameters Œ∏ remain frozen.
        
        This interface matches other models in the BMA framework.
        
        Args:
            returns: Array of observed returns
            prior_mean: Prior mean for regularization (not used for flow)
            prior_lambda: Prior strength (not used for flow)
            
        Returns:
            Dictionary with optimized parameters and diagnostics
        """
        # Run filter
        filter_result = self.filter(returns)
        
        # Compute BIC for model selection
        n_params = self.config.latent_dim + 2  # z dimension + EWMA state
        n_obs = len(returns)
        bic = -2 * filter_result['total_log_likelihood'] + n_params * np.log(n_obs)
        aic = -2 * filter_result['total_log_likelihood'] + 2 * n_params
        
        return {
            # Standard BMA interface
            'mean_log_likelihood': filter_result['mean_log_likelihood'],
            'bic': bic,
            'aic': aic,
            'n_params': n_params,
            'n_observations': n_obs,
            'fit_success': True,
            
            # AIGF-NF specific
            'latent_z': filter_result['latent_z'],
            'latent_dim': filter_result['latent_dim'],
            'clipping_frequency': filter_result['clipping_frequency'],
            'stability_violations': filter_result['stability_violations'],
            'predictive_mean': filter_result['predictive_mean'],
            'predictive_std': filter_result['predictive_std'],
            'tail_heaviness': filter_result['tail_heaviness'],
            'novelty_score': filter_result['novelty_score'],
            
            # Config snapshot
            'config': self.config.to_dict(),
            'flow_artifact_id': self.flow_artifact.artifact_id,
            'flow_version': self.flow_artifact.version,
        }
    
    def pit_ks(self, returns: np.ndarray) -> Tuple[float, float]:
        """
        Compute PIT-based KS statistic for calibration.
        
        Args:
            returns: Array of observed returns
            
        Returns:
            Tuple of (KS statistic, KS p-value)
        """
        calibration = self._compute_calibration(returns)
        return calibration.pit_ks_statistic, calibration.pit_ks_pvalue
    
    # =========================================================================
    # PERSISTENCE
    # =========================================================================
    
    def to_dict(self) -> Dict[str, Any]:
        """Export full model state for persistence."""
        return {
            'config': self.config.to_dict(),
            'flow_artifact': self.flow_artifact.to_dict(),
            'latent_state': self.state.to_dict(),
        }
    
    @classmethod
    def from_dict(cls, d: Dict[str, Any], rng: Optional[np.random.Generator] = None) -> 'AIGFNFModel':
        """Load model from persisted state."""
        config = AIGFNFConfig.from_dict(d['config'])
        flow_artifact = FlowArtifact.from_dict(d['flow_artifact'])
        latent_state = LatentState.from_dict(d['latent_state'])
        
        return cls(
            config=config,
            flow_artifact=flow_artifact,
            latent_state=latent_state,
            rng=rng,
        )


# =============================================================================
# MODEL NAME GENERATION ‚Äî Registry Integration
# =============================================================================

AIGF_NF_MODEL_NAME = "aigf_nf"


def get_aigf_nf_model_name() -> str:
    """Generate canonical name for AIGF-NF model."""
    return AIGF_NF_MODEL_NAME


def is_aigf_nf_model(model_name: str) -> bool:
    """Check if a model name corresponds to AIGF-NF."""
    if not model_name:
        return False
    return model_name.lower() == AIGF_NF_MODEL_NAME.lower()


# =============================================================================
# FACTORY FUNCTIONS
# =============================================================================

def create_aigf_nf_model(
    config: Optional[AIGFNFConfig] = None,
    flow_artifact_path: Optional[str] = None,
    seed: Optional[int] = None,
) -> AIGFNFModel:
    """
    Factory function to create AIGF-NF model.
    
    Args:
        config: Model configuration (uses default if None)
        flow_artifact_path: Path to pretrained flow artifact (creates default if None)
        seed: Random seed for reproducibility
        
    Returns:
        Initialized AIGF-NF model
    """
    rng = np.random.default_rng(seed)
    
    flow_artifact = None
    if flow_artifact_path is not None and os.path.exists(flow_artifact_path):
        with open(flow_artifact_path, 'r') as f:
            flow_artifact = FlowArtifact.from_dict(json.load(f))
    
    return AIGFNFModel(
        config=config,
        flow_artifact=flow_artifact,
        rng=rng,
    )


def fit_aigf_nf(
    returns: np.ndarray,
    config: Optional[AIGFNFConfig] = None,
    seed: Optional[int] = None,
) -> Dict[str, Any]:
    """
    Convenience function to fit AIGF-NF model to returns.
    
    Args:
        returns: Array of observed returns
        config: Model configuration
        seed: Random seed
        
    Returns:
        Dictionary with fit results
    """
    model = create_aigf_nf_model(config=config, seed=seed)
    return model.optimize_params(returns)


# =============================================================================
# EXPORTS
# =============================================================================

__all__ = [
    # Configuration
    'AIGFNFConfig',
    'DEFAULT_AIGF_NF_CONFIG',
    
    # Flow Artifact
    'FlowArtifact',
    
    # State
    'LatentState',
    
    # Results
    'InterpretabilityProxies',
    'StabilityDiagnostics',
    'AIGFNFCalibrationResult',
    'AIGFNFPredictiveResult',
    
    # Model
    'AIGFNFModel',
    
    # Name generation
    'AIGF_NF_MODEL_NAME',
    'get_aigf_nf_model_name',
    'is_aigf_nf_model',
    
    # Factory
    'create_aigf_nf_model',
    'fit_aigf_nf',
]

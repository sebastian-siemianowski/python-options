<h1 align="center">Quantitative Signal Engine</h1>

<p align="center">
  <strong>Where Bayesian Model Averaging meets Kalman Filtering</strong><br>
  <sub>Multi-asset signal generation with calibrated uncertainty</sub>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/python-3.7+-blue.svg" alt="Python 3.7+">
  <img src="https://img.shields.io/badge/platform-macOS-lightgrey.svg" alt="macOS">
  <img src="https://img.shields.io/badge/assets-100+-green.svg" alt="100+ Assets">
  <img src="https://img.shields.io/badge/models-7_per_regime-orange.svg" alt="7 Models">
</p>

<p align="center">
  <a href="#the-system">The System</a> ‚Ä¢
  <a href="#quick-start">Quick Start</a> ‚Ä¢
  <a href="#daily-workflow">Daily Workflow</a> ‚Ä¢
  <a href="#command-reference">Commands</a> ‚Ä¢
  <a href="#the-mathematics">Mathematics</a> ‚Ä¢
  <a href="#architecture">Architecture</a>
</p>

---

## Why This System Exists

Most trading systems choose a single model and pretend it's correct. This system doesn't.

Instead, it maintains **7 competing models** across **5 market regimes**, letting Bayesian inference continuously update which models are most credible given recent data. Signals emerge from the **full posterior predictive distribution**‚Äînot from any single "best guess."

The result: **calibrated uncertainty**. When the system says "62% probability of positive return," it means that historically, 62% of such predictions were correct.

> *"The goal is not to be right. The goal is to know how confident you should be."*

---

## The System

This is a **belief evolution engine**, not a rule engine.

At its core, the system maintains a population of competing models‚Äîeach representing a different hypothesis about market dynamics. These models evolve in probability over time through Bayesian updating, and signals emerge from the full predictive distribution, not from point estimates.

### The Pipeline

```
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                                                                                       ‚ïë
‚ïë   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                                     ‚ïë
‚ïë   ‚îÇ Yahoo       ‚îÇ                                                                     ‚ïë
‚ïë   ‚îÇ Finance API ‚îÇ                                                                     ‚ïë
‚ïë   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                                     ‚ïë
‚ïë          ‚îÇ                                                                            ‚ïë
‚ïë          ‚ñº                                                                            ‚ïë
‚ïë   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚ïë
‚ïë   ‚îÇ                         DATA ENGINE  (make data)                            ‚îÇ     ‚ïë
‚ïë   ‚îÇ                                                                             ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚Ä¢ Fetch 10 years OHLCV for 100+ symbols                                   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚Ä¢ Multi-pass retry (Yahoo is flaky)                                       ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚Ä¢ Incremental cache updates                                               ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚Ä¢ Currency conversion to PLN base                                         ‚îÇ     ‚ïë
‚ïë   ‚îÇ                                                                             ‚îÇ     ‚ïë
‚ïë   ‚îÇ   Output: data/{SYMBOL}_1d.csv                                              ‚îÇ     ‚ïë
‚ïë   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚ïë
‚ïë                                      ‚îÇ                                                ‚ïë
‚ïë                                      ‚ñº                                                ‚ïë
‚ïë   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚ïë
‚ïë   ‚îÇ                        TUNING ENGINE  (make tune)                           ‚îÇ     ‚ïë
‚ïë   ‚îÇ                                                                             ‚îÇ     ‚ïë
‚ïë   ‚îÇ   For each asset:                                                           ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ  For each regime r ‚àà {LOW_VOL_TREND, HIGH_VOL_TREND,                ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ                       LOW_VOL_RANGE, HIGH_VOL_RANGE, CRISIS_JUMP}:  ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ                                                                     ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ    For each model m ‚àà {kalman_gaussian,                             ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ                        kalman_phi_gaussian,                         ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ                        phi_student_t_nu_4,  phi_student_t_nu_6,     ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ                        phi_student_t_nu_8,  phi_student_t_nu_12,    ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ                        phi_student_t_nu_20}:                        ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ                                                                     ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ      1. Fit Œ∏ = {q, c, œÜ} via MLE with regularization prior         ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ      2. Compute log-likelihood ‚Ñì(Œ∏)                                 ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ      3. Compute BIC = -2‚Ñì + k¬∑log(n)                                ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ      4. Compute Hyv√§rinen score (robust to misspecification)        ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ      5. Run PIT calibration diagnostics                             ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ                                                                     ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ    Aggregate across models:                                         ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ      ‚Ä¢ w(m|r) = exp(-¬Ω ¬∑ ŒîBIC) ¬∑ hyv_weight^(1-Œ±)                   ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ      ‚Ä¢ Apply temporal smoothing: w ‚Üê w_prev^Œ± ¬∑ w_raw               ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ      ‚Ä¢ Apply hierarchical shrinkage toward global                   ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ      ‚Ä¢ Normalize: p(m|r) = w(m|r) / Œ£w                              ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ     ‚ïë
‚ïë   ‚îÇ                                                                             ‚îÇ     ‚ïë
‚ïë   ‚îÇ   Output: scripts/quant/cache/kalman_q_cache.json                           ‚îÇ     ‚ïë
‚ïë   ‚îÇ           {asset: {regime: {model: {q, œÜ, ŒΩ, BIC, p(m|r), ...}}}}           ‚îÇ     ‚ïë
‚ïë   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚ïë
‚ïë                                      ‚îÇ                                                ‚ïë
‚ïë                                      ‚ñº                                                ‚ïë
‚ïë   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚ïë
‚ïë   ‚îÇ                       SIGNAL ENGINE  (make stocks)                          ‚îÇ     ‚ïë
‚ïë   ‚îÇ                                                                             ‚îÇ     ‚ïë
‚ïë   ‚îÇ   For each asset:                                                           ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ  1. REGIME DETECTION                                                ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ     ‚Ä¢ Compute rolling volatility (EWMA fast/slow blend)             ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ     ‚Ä¢ Compute drift magnitude                                       ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ     ‚Ä¢ Classify: r_t ‚àà {0,1,2,3,4}                                   ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ                                                                     ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ  2. LOAD BELIEFS                                                    ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ     ‚Ä¢ Retrieve p(m|r_t) and Œ∏_{r_t,m} from cache                    ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ     ‚Ä¢ If regime sparse ‚Üí borrow from global (hierarchical)          ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ                                                                     ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ  3. POSTERIOR PREDICTIVE MONTE CARLO                                ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ     samples = []                                                    ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ     for m, weight in p(m|r_t):                                      ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ         n_samples = weight √ó N_total                                ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ         for each sample:                                            ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ             Œº = kalman_drift_estimate                               ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ             for t in 1..horizon:                                    ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ                 Œº ‚Üê œÜ¬∑Œº + Œ∑,  Œ∑ ~ N(0, q)                           ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ                 r_t ‚Üê Œº + Œµ,  Œµ ~ model_distribution(œÉ)             ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ             samples.append(Œ£ r_t)                                   ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ                                                                     ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ  4. DECISION LAYER                                                  ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ     ‚Ä¢ P(return > 0) = count(samples > 0) / N                        ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ     ‚Ä¢ E[return] = mean(samples)                                     ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ     ‚Ä¢ Apply exhaustion dampening (UE‚Üë/UE‚Üì)                          ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îÇ     ‚Ä¢ Map: P > 58% ‚Üí BUY, P < 42% ‚Üí SELL, else ‚Üí HOLD               ‚îÇ   ‚îÇ     ‚ïë
‚ïë   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ     ‚ïë
‚ïë   ‚îÇ                                                                             ‚îÇ     ‚ïë
‚ïë   ‚îÇ   Output: Console tables + cached JSON                                      ‚îÇ     ‚ïë
‚ïë   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚ïë
‚ïë                                      ‚îÇ                                                ‚ïë
‚ïë                                      ‚ñº                                                ‚ïë
‚ïë                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                  ‚ïë
‚ïë                        ‚îÇ   BUY  ‚îÇ  HOLD  ‚îÇ  SELL   ‚îÇ                                  ‚ïë
‚ïë                        ‚îÇ   üü¢   ‚îÇ   ‚ö™   ‚îÇ   üî¥   ‚îÇ                                   ‚ïë
‚ïë                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                  ‚ïë
‚ïë                                                                                       ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
```

### Quick Reference

| Engine | Command | Input | Output | Time |
|--------|---------|-------|--------|------|
| **Data** | `make data` | Yahoo Finance API | `data/*.csv` | 5-15 min |
| **Tuning** | `make tune` | Price CSVs | `kalman_q_cache.json` | 2-10 min |
| **Signal** | `make stocks` | Cache + fresh prices | Console + JSON | 1-3 min |

### Asset Universe

The system tracks **100+ assets** across multiple asset classes:

| Class | Examples | Count |
|-------|----------|-------|
| **Equities** | AAPL, MSFT, NVDA, TSLA, JPM, GS, UNH, LLY... | ~80 |
| **Defense** | LMT, RTX, NOC, GD, BA, HII, AVAV, PLTR... | ~40 |
| **ETFs** | SPY, VOO, GLD, SLV, SMH | 5 |
| **Commodities** | GC=F (Gold), SI=F (Silver) | 2 |
| **Crypto** | BTC-USD, MSTR | 2 |
| **FX** | PLNJPY=X | 1 |

All prices are converted to a common base currency (PLN) for portfolio-level analysis.

### Model Universe

The Tuning Engine fits **7 model classes** per regime:

| Model | Parameters | Use Case |
|-------|------------|----------|
| `kalman_gaussian` | q, c | Baseline Gaussian innovations |
| `kalman_phi_gaussian` | q, c, œÜ | AR(1) drift with Gaussian |
| `phi_student_t_nu_4` | q, c, œÜ | Heavy tails (ŒΩ=4) |
| `phi_student_t_nu_6` | q, c, œÜ | Moderate tails (ŒΩ=6) |
| `phi_student_t_nu_8` | q, c, œÜ | Light tails (ŒΩ=8) |
| `phi_student_t_nu_12` | q, c, œÜ | Near-Gaussian (ŒΩ=12) |
| `phi_student_t_nu_20` | q, c, œÜ | Almost Gaussian (ŒΩ=20) |

Student-t models use a **discrete ŒΩ grid** (not continuous optimization). Each ŒΩ is a separate sub-model in BMA, allowing the posterior to express uncertainty about tail thickness.

### Regime Classification

Markets are classified into **5 regimes** based on volatility and drift:

| Regime | Condition |
|--------|-----------|
| `LOW_VOL_TREND` | vol < 0.85√ómedian, \|drift\| > threshold |
| `HIGH_VOL_TREND` | vol > 1.3√ómedian, \|drift\| > threshold |
| `LOW_VOL_RANGE` | vol < 0.85√ómedian, \|drift\| ‚â§ threshold |
| `HIGH_VOL_RANGE` | vol > 1.3√ómedian, \|drift\| ‚â§ threshold |
| `CRISIS_JUMP` | vol > 2√ómedian OR tail_indicator > 4 |

Regime assignment is **deterministic and consistent** between tuning and inference.

---

## Quick Start

### Prerequisites

- macOS (Intel or Apple Silicon)
- Python 3.7+
- ~10GB disk space for price cache

### Installation (One Command)

```bash
make setup
```

This will:
1. Create `.venv/` virtual environment
2. Install dependencies from `requirements.txt`
3. Download 10 years of price data (3 passes for reliability)
4. Clean cached data

**Time:** 5-15 minutes depending on network.

### Generate Your First Signals

```bash
make stocks
```

### What You'll See

The system outputs beautifully formatted Rich console tables:

```
‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÇ                           NVDA ‚Äî NVIDIA Corporation                       ‚îÇ
‚îÇ                      Regime: LOW_VOL_TREND ‚îÇ Current: $142.58             ‚îÇ
‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ

 Horizon     P(r>0)    E[return]    Signal     Confidence
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
 1 day       54.2%      +0.08%      HOLD       ‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë
 1 week      58.7%      +0.42%      BUY        ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë
 1 month     63.1%      +1.84%      BUY        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë
 3 months    71.2%      +5.62%      BUY        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë
 12 months   78.4%     +18.41%      BUY        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë
```

Signals are color-coded:
- üü¢ **BUY** (green): P(r>0) ‚â• 58%
- ‚ö™ **HOLD** (dim): P(r>0) ‚àà (42%, 58%)
- üî¥ **SELL** (red): P(r>0) ‚â§ 42%

### Understanding the Columns

| Column | Meaning |
|--------|---------|
| **Horizon** | Forecast period (trading days) |
| **P(r>0)** | Probability that return will be positive |
| **E[return]** | Expected log return from posterior mean |
| **Signal** | Decision derived from probability threshold |
| **Confidence** | Visual indicator of probability magnitude |

### Understanding the Regime

Each asset is classified into one of 5 regimes:

| Regime | What It Means | Typical Behavior |
|--------|---------------|------------------|
| `LOW_VOL_TREND` | Quiet trending market | Smooth, directional moves |
| `HIGH_VOL_TREND` | Volatile trending market | Sharp moves with direction |
| `LOW_VOL_RANGE` | Quiet range-bound | Mean-reverting, choppy |
| `HIGH_VOL_RANGE` | Volatile range-bound | Whipsaw, no clear direction |
| `CRISIS_JUMP` | Extreme stress | Tail events, correlations spike |

The regime affects which model receives the most weight in the BMA mixture.

---

## Daily Workflow

### The 30-Second Morning Routine

```bash
make stocks
```

That's it. This single command:
1. Refreshes the last 5 days of price data
2. Loads cached Kalman parameters
3. Generates signals for all assets
4. Displays formatted output

### When to Re-Tune

The Tuning Engine should be run:
- **Weekly** during normal markets
- **After major regime shifts** (VIX spike, Fed announcement)
- **When signals feel stale** or miscalibrated

```bash
# Weekly calibration
make tune

# Force complete re-estimation (ignore cache)
make tune ARGS="--force"
```

### Offline Mode

Already have cached data? Work without network:

```bash
# Render from cache only
make report

# Or set environment variable
OFFLINE_MODE=1 make stocks
```

### Quick Validation

Before trusting signals, validate calibration:

```bash
# Check if probabilities match historical outcomes
make fx-calibration

# Quick smoke test with 20 assets
make top20
```

---

## Command Reference

All interaction happens through `make`. The Makefile orchestrates Python scripts, manages the virtual environment, and handles caching transparently.

---

### üöÄ Setup & Installation

#### `make setup`

**The one command to rule them all.** Run this once after cloning.

```bash
make setup
```

**What happens internally:**
1. Creates `.venv/` via `setup_venv.sh`
2. Upgrades pip and installs `requirements.txt`
3. Runs `precache_data.py` **3 times** (Yahoo Finance is flaky)
4. Runs `clean_cache.py` to remove empty rows

**Time:** 5-15 minutes  
**Disk:** ~10GB for full price cache

#### `make doctor`

Reinstalls all dependencies. Use when imports fail or packages are corrupted.

```bash
make doctor
```

---

### üìä Data Engine

The Data Engine fetches OHLCV (Open, High, Low, Close, Volume) from Yahoo Finance and caches locally as CSV files.

#### `make data`

Downloads full price history for all assets in the universe.

```bash
make data                              # Standard run
make data ARGS="--workers 4"           # Reduce parallelism
make data ARGS="--batch-size 8"        # Smaller batches
```

**Internals:**
- Runs `refresh_data.py --skip-trim --retries 5 --workers 12 --batch-size 16`
- 5 retry passes (Yahoo Finance rate-limits aggressively)
- 12 parallel workers, 16 assets per batch
- Output: `data/<SYMBOL>_1d.csv`

#### `make refresh`

Updates only the last 5 days. Fast daily refresh.

```bash
make refresh
```

**Internals:**
- Deletes last 5 rows from each cache file
- Re-downloads with 5 retry passes
- Typical time: 2-5 minutes

#### `make clean-cache`

Removes rows with all-NaN values (dates before asset existed).

```bash
make clean-cache
```

#### `make failed`

Lists assets that failed to download (stored in `scripts/fx_failures.json`).

```bash
make failed
```

#### `make purge`

Deletes cache files for failed assets so they can be re-downloaded.

```bash
make purge                    # Clear cache for failed assets
make purge ARGS="--all"       # Also clear the failures list
```

---

### üîß Tuning Engine

The Tuning Engine estimates Kalman filter parameters via Maximum Likelihood Estimation, then applies Bayesian Model Averaging across model classes.

#### `make tune`

The heart of the calibration system.

```bash
make tune                              # Uses cache, skips already-tuned assets
make tune ARGS="--force"               # Re-estimate everything
make tune ARGS="--max-assets 10"       # Test with subset
make tune ARGS="--dry-run"             # Preview without executing
```

**What happens internally:**
1. Loads asset universe from `fx_data_utils.py`
2. For each asset, for each of 5 regimes:
   - Fits 7 model classes (Gaussian, AR(1)-Gaussian, Student-t with ŒΩ ‚àà {4,6,8,12,20})
   - Computes BIC, AIC, Hyv√§rinen score, PIT diagnostics
   - Converts scores to posterior weights
   - Applies temporal smoothing against previous run
   - Applies hierarchical shrinkage toward global
3. Saves to `scripts/quant/cache/kalman_q_cache.json`

**Key ARGS:**
| Argument | Description | Default |
|----------|-------------|---------|
| `--force` | Ignore cache, re-estimate all | False |
| `--max-assets N` | Process only first N assets | All |
| `--dry-run` | Preview without processing | False |
| `--prior-mean X` | Prior mean for log‚ÇÅ‚ÇÄ(q) | -6.0 |
| `--prior-lambda X` | Regularization strength | 1.0 |
| `--lambda-regime X` | Hierarchical shrinkage | 0.05 |
| `--debug` | Show stack traces on errors | False |

#### `make show-q`

Displays the cached Kalman parameters as raw JSON.

```bash
make show-q
```

#### `make clear-q`

Deletes the parameter cache. Next `make tune` will re-estimate everything.

```bash
make clear-q
```

---

### üìà Signal Engine

The Signal Engine consumes tuned parameters and generates Buy/Hold/Sell signals via posterior predictive Monte Carlo.

#### `make stocks`

**The main command for daily use.** Refreshes data, then generates signals.

```bash
make stocks                            # Full pipeline
make stocks ARGS="--assets AAPL,MSFT"  # Specific assets only
```

**What happens internally:**
1. Runs `refresh_data.py` (updates last 5 days)
2. Runs `fx_pln_jpy_signals.py` with caching enabled
3. For each asset:
   - Determines current regime r_t
   - Loads model posterior p(m|r_t) from cache
   - Runs posterior predictive Monte Carlo
   - Computes expected utility across horizons
   - Maps to BUY/HOLD/SELL

**Output:** Beautiful Rich console tables showing:
- Signal per horizon (1d, 3d, 7d, 21d, 63d, 126d, 252d)
- Probability of positive return
- Expected log return
- Confidence indicators

#### `make report`

Renders signals from cache without network calls. Use when offline.

```bash
make report
```

#### `make top20`

Quick smoke test with first 20 assets. Good for testing changes.

```bash
make top20
```

---

### üî¨ Diagnostic Commands

These commands validate model quality and calibration.

#### `make fx-diagnostics`

Full diagnostic suite. **Expensive** (runs out-of-sample tests).

```bash
make fx-diagnostics
```

**Includes:**
- Log-likelihood analysis
- Parameter stability across time windows
- Out-of-sample predictive tests

#### `make fx-diagnostics-lite`

Lightweight diagnostics. Skips OOS tests.

```bash
make fx-diagnostics-lite
```

#### `make fx-calibration`

Probability Integral Transform (PIT) calibration check.

```bash
make fx-calibration
```

**What it tests:** If your 60% confidence intervals contain outcomes 60% of the time.

#### `make fx-model-comparison`

Compares model classes via AIC/BIC. Shows which models the posterior favors.

```bash
make fx-model-comparison
```

#### `make fx-validate-kalman`

Level-7 Kalman validation science:
- Drift estimation accuracy
- Likelihood surface analysis
- PIT histogram uniformity
- Stress regime behavior

```bash
make fx-validate-kalman                        # Console output only
make fx-validate-kalman-plots                  # Also saves plots to plots/kalman_validation/
```

#### `make tests`

Runs the unit test suite.

```bash
make tests
```

---

### üí∞ Debt Allocation Engine

A specialized decision engine for balance-sheet currency risk.

#### `make debt`

Determines the optimal day to switch JPY-denominated debt to EUR-denominated debt.

```bash
make debt
```

**This is NOT a trade signal.** It's a corporate treasury tool for:
- Balance-sheet convexity control
- Latent state inference (NORMAL ‚Üí COMPRESSED ‚Üí PRE_POLICY ‚Üí POLICY)
- Auditable, causal decision logic

Output: `scripts/quant/cache/debt/`

---

### üìã Options Screener & Backtesting

Legacy modules for equity options analysis.

#### `make run`

Runs the options screener with support/resistance analysis.

```bash
make run                                       # Uses tickers.csv
make run ARGS="--tickers AAPL,MSFT,NVDA"       # Explicit tickers
make run ARGS="--min_oi 200 --min_vol 50"      # Filter thresholds
```

**Output:**
- `screener_results.csv`
- `plots/<TICKER>_support_resistance.png`

#### `make backtest`

Multi-year strategy simulation with Black-Scholes pricing.

```bash
make backtest                                  # Uses tickers.csv
make backtest ARGS="--tickers AAPL --bt_years 5"
```

**Key ARGS:**
| Argument | Description | Default |
|----------|-------------|---------|
| `--bt_years` | Years of history | 3 |
| `--bt_dte` | Days to expiration | 7 |
| `--bt_moneyness` | OTM percent | 0.05 |
| `--bt_tp_x` | Take-profit multiple | None |
| `--bt_sl_x` | Stop-loss multiple | None |
| `--bt_alloc_frac` | Equity fraction per trade | 0.1 |

**Output:**
- `backtests/<TICKER>_equity.csv`
- `screener_results_backtest.csv`

---

### üìä Fundamental Screeners

#### `make top50`

Ranks small/mid caps by 3-year revenue CAGR.

```bash
make top50
make top50 ARGS="--csv path/to/universe.csv"
```

#### `make bagger50`

Ranks by 100√ó Bagger Score (probability-weighted growth potential).

```bash
make bagger50
make bagger50 ARGS="--bagger_horizon 15"       # 15-year horizon
make bagger50 ARGS="--bagger_verbose"          # Show sub-scores
```

#### `make top100`

Top 100 screener using Russell 5000 universe.

```bash
make top100
```

#### `make build-russell`

Builds `data/universes/russell2500_tickers.csv` from public sources.

```bash
make build-russell
```

#### `make russell5000`

Builds the larger Russell 5000 universe.

```bash
make russell5000
```

---

### üßπ Utility Commands

#### `make clear`

Nuclear option. Clears all caches, plots, and temp files.

```bash
make clear
```

**Deletes:**
- `__pycache__/`
- `plots/*.png`
- `data/meta/`
- `data/*.backup`

#### `make colors`

Displays color palette test. Useful for terminal configuration.

```bash
make colors
```

---

## Architecture

```
python-options/
‚îÇ
‚îú‚îÄ‚îÄ Makefile                    # Command interface (start here)
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ tune_q_mle.py           # TUNING ENGINE: MLE + BMA
‚îÇ   ‚îú‚îÄ‚îÄ tune_pretty.py          # Tuning UX wrapper
‚îÇ   ‚îú‚îÄ‚îÄ fx_pln_jpy_signals.py   # SIGNAL ENGINE: Posterior predictive
‚îÇ   ‚îú‚îÄ‚îÄ fx_signals_presentation.py  # Rich console output
‚îÇ   ‚îú‚îÄ‚îÄ refresh_data.py         # DATA ENGINE: Bulk download
‚îÇ   ‚îú‚îÄ‚îÄ fx_data_utils.py        # Data utilities + caching
‚îÇ   ‚îú‚îÄ‚îÄ debt_allocator.py       # Debt switch decision engine
‚îÇ   ‚îî‚îÄ‚îÄ quant/
‚îÇ       ‚îî‚îÄ‚îÄ cache/
‚îÇ           ‚îî‚îÄ‚îÄ kalman_q_cache.json  # Tuned parameters
‚îÇ
‚îú‚îÄ‚îÄ data/                       # Price cache (CSV per symbol)
‚îú‚îÄ‚îÄ options.py                  # Options screener
‚îú‚îÄ‚îÄ backtests/                  # Equity curves
‚îî‚îÄ‚îÄ plots/                      # Generated charts
```

### Design Principles

1. **Separation of concerns**
   - Tuning engine knows nothing about decisions
   - Signal engine acts on beliefs, doesn't create them
   - Presentation layer is fully decoupled

2. **Bayesian integrity**
   - When evidence is weak, the system becomes more ignorant, not more confident
   - Fallback is hierarchical: `p(m|r, weak data) ‚Üí p(m|global)`
   - Never synthesize beliefs that weren't learned

3. **Auditability**
   - All parameters cached and versioned
   - No hidden state mutations
   - Deterministic regime assignment

---

## The Mathematics

> *"The math always emerges from the underlying system‚Äînot the other way around."*

This section documents the mathematical foundations that govern each engine. The code implements these equations; understanding them illuminates why the system behaves as it does.

---

### Data Engine: Returns and Volatility

**Log Returns**

The system works with log returns, not simple returns:

```
r‚Çú = log(P‚Çú / P‚Çú‚Çã‚ÇÅ)
```

Log returns are additive over time and approximately normal for small values, which simplifies the probabilistic machinery.

**Realized Volatility**

Volatility is estimated via exponentially-weighted moving average (EWMA):

```
œÉ‚Çú¬≤ = Œª ¬∑ œÉ‚Çú‚Çã‚ÇÅ¬≤ + (1 - Œª) ¬∑ r‚Çú¬≤
```

Where Œª ‚àà (0,1) controls decay. We use multiple speeds:
- **Fast** (Œª = 0.94): Responsive to recent moves
- **Slow** (Œª = 0.97): Smoother, less reactive

The final volatility blends both for robustness.

**Winsorization**

Extreme returns are clipped to reduce outlier influence:

```
r‚Çú ‚Üí clip(r‚Çú, -3œÉ, +3œÉ)
```

This makes parameter estimation more stable without discarding information entirely.

---

### Tuning Engine: Kalman Filter + MLE

**The State-Space Model**

We model latent drift Œº‚Çú as a random walk observed through noisy returns:

```
State equation:     Œº‚Çú = Œº‚Çú‚Çã‚ÇÅ + Œ∑‚Çú,     Œ∑‚Çú ~ N(0, q)
Observation:        r‚Çú = Œº‚Çú + Œµ‚Çú,       Œµ‚Çú ~ N(0, œÉ‚Çú¬≤)
```

Here:
- **Œº‚Çú** is the unobserved "true" drift
- **q** is the **process noise variance** (how much drift can change per step)
- **œÉ‚Çú¬≤** is the observation noise (market volatility)

**Kalman Filter Recursion**

Given prior Œº‚Çú‚Çã‚ÇÅ|‚Çú‚Çã‚ÇÅ ~ N(m, P), the Kalman filter updates:

```
Predict:    Œº‚Çú|‚Çú‚Çã‚ÇÅ ~ N(m, P + q)

Update:     K = (P + q) / (P + q + œÉ‚Çú¬≤)           # Kalman gain
            m‚Çú = m + K ¬∑ (r‚Çú - m)                  # Posterior mean
            P‚Çú = (1 - K) ¬∑ (P + q)                 # Posterior variance
```

The Kalman gain K ‚àà (0,1) balances prior belief against new evidence.

**Maximum Likelihood Estimation**

We find q by maximizing the log-likelihood:

```
‚Ñì(q) = Œ£‚Çú log p(r‚Çú | r‚ÇÅ:‚Çú‚Çã‚ÇÅ, q)
     = -¬Ω Œ£‚Çú [ log(2œÄ ¬∑ v‚Çú) + (r‚Çú - m‚Çú)¬≤ / v‚Çú ]
```

Where v‚Çú = P + q + œÉ‚Çú¬≤ is the predictive variance.

**Regularization Prior**

To prevent overfitting, we add a Gaussian prior on log‚ÇÅ‚ÇÄ(q):

```
log‚ÇÅ‚ÇÄ(q) ~ N(Œº_prior, 1/Œª)
```

Default: Œº_prior = -6 (q ‚âà 10‚Åª‚Å∂), Œª = 1.0

The penalized objective becomes:

```
‚Ñì_penalized(q) = ‚Ñì(q) - Œª/2 ¬∑ (log‚ÇÅ‚ÇÄ(q) - Œº_prior)¬≤
```

**AR(1) Extension (œÜ-models)**

For mean-reverting drift, we extend the state equation:

```
Œº‚Çú = œÜ ¬∑ Œº‚Çú‚Çã‚ÇÅ + Œ∑‚Çú,     œÜ ‚àà (-1, 1)
```

When |œÜ| < 1, drift reverts toward zero. We apply a shrinkage prior:

```
œÜ ~ N(0, œÑ¬≤)
```

This prevents unit-root instability (œÜ ‚Üí 1).

**Student-t Innovations**

To capture fat tails, we replace Gaussian innovations with Student-t:

```
Œµ‚Çú ~ t_ŒΩ(0, œÉ‚Çú)
```

The degrees-of-freedom ŒΩ controls tail thickness:
- ŒΩ = 4: Very heavy tails
- ŒΩ = 20: Nearly Gaussian
- ŒΩ ‚Üí ‚àû: Gaussian limit

We use a discrete grid ŒΩ ‚àà {4, 6, 8, 12, 20} and let BMA select the mixture.

---

### Tuning Engine: Bayesian Model Averaging

**The BMA Equation**

Given regime r and model class m with parameters Œ∏, the posterior predictive is:

```
p(r‚Çú‚Çä‚Çï | r) = Œ£‚Çò p(r‚Çú‚Çä‚Çï | r, m, Œ∏·µ£,‚Çò) ¬∑ p(m | r)
```

This is the **core equation** of the system. Signals emerge from this mixture, not from any single "best" model.

**Model Weights via BIC**

For each model m in regime r, we compute BIC:

```
BIC_m,r = -2 ¬∑ ‚Ñì_m,r + k_m ¬∑ log(n_r)
```

Where:
- ‚Ñì_m,r = maximized log-likelihood
- k_m = number of parameters
- n_r = sample size in regime r

Weights are softmax over negative BIC:

```
w_raw(m|r) = exp(-¬Ω ¬∑ (BIC_m,r - BIC_min,r))
p(m|r) = w_raw(m|r) / Œ£‚Çò' w_raw(m'|r)
```

**Hyv√§rinen Score (Robust Alternative)**

BIC assumes the true model is in the candidate set. When misspecified, the **Hyv√§rinen score** is more robust:

```
H(m) = Œ£‚Çú [ ‚àÇ¬≤log p / ‚àÇr¬≤ + ¬Ω(‚àÇlog p / ‚àÇr)¬≤ ]
```

This is a **proper scoring rule** that doesn't require normalizing constants and naturally rewards tail accuracy.

We blend BIC and Hyv√§rinen:

```
w_combined(m) = w_bic(m)^Œ± ¬∑ w_hyvarinen(m)^(1-Œ±)
```

Default Œ± = 0.5.

**Temporal Smoothing**

To prevent erratic model switching, we smooth weights over time:

```
w_smooth(m|r) ‚àù w_prev(m|r)^Œ± ¬∑ w_raw(m|r)
```

With Œ± ‚âà 0.85, this creates "sticky" posteriors that adapt gradually.

**Hierarchical Shrinkage**

When regime r has few samples, we shrink toward the global posterior:

```
p(m|r) = (1 - Œª) ¬∑ p_local(m|r) + Œª ¬∑ p(m|global)
```

Default Œª = 0.05. When samples < threshold, we set Œª = 1 (full borrowing) and mark `borrowed_from_global = True`.

---

### Signal Engine: Posterior Predictive Monte Carlo

**Monte Carlo Sampling**

We approximate p(r‚Çú‚Çä‚Çï | r_t) via simulation:

```python
samples = []
for m, w in model_posterior.items():
    n_m = int(w * N_total)  # samples proportional to weight
    for _ in range(n_m):
        # Simulate Kalman path for h steps
        Œº = current_drift_estimate
        for step in range(h):
            Œº += sample_from(N(0, q_m))
            r_step = Œº + sample_from(distribution_m(œÉ))
        samples.append(sum_of_r_steps)
```

This produces samples from the full BMA mixture, not from any single model.

**Probability of Positive Return**

From the sample distribution:

```
P(r‚Çú‚Çä‚Çï > 0) = (# samples > 0) / N_total
```

This is the key quantity for BUY/HOLD/SELL decisions.

**Expected Log Return**

```
E[r‚Çú‚Çä‚Çï] = mean(samples)
```

Used for position sizing and expected utility calculations.

**Signal Mapping**

Signals map from probability:

```
P(r > 0) ‚â• 0.58  ‚Üí  BUY
P(r > 0) ‚àà (0.42, 0.58)  ‚Üí  HOLD
P(r > 0) ‚â§ 0.42  ‚Üí  SELL
```

The 58%/42% thresholds derive from expected utility theory with symmetric loss.

---

### Signal Engine: Expected Utility

**The EU Framework**

Decisions maximize expected utility, not expected return:

```
EU = p ¬∑ U(gain) + (1-p) ¬∑ U(loss)
```

For Kelly-style sizing with log utility U(x) = log(1 + x):

```
f* = p - (1-p)/b
```

Where:
- f* = optimal fraction of capital
- p = probability of win
- b = win/loss ratio

**Risk-Adjusted Edge**

We compute a Sharpe-style z-score:

```
z = (Œº / œÉ) ¬∑ ‚àöh
```

Where h is the horizon in days. This normalizes edge across timeframes.

**Volatility Regime Dampening**

In high-volatility regimes, we reduce conviction:

```
z_adj = z ¬∑ (1 - vol_penalty)
vol_penalty = max(0, (œÉ / œÉ_median - 1.5) ¬∑ 0.3)
```

This prevents overconfidence when uncertainty is elevated.

---

### Debt Engine: Latent State Model

**State Space**

The debt allocator models policy stress via 4 latent states:

```
S ‚àà {NORMAL, COMPRESSED, PRE_POLICY, POLICY}
```

States are **partially ordered**: NORMAL ‚Üí COMPRESSED ‚Üí PRE_POLICY ‚Üí POLICY. Backward transitions are forbidden except via explicit reset.

**Observation Model**

We observe a 5-dimensional feature vector:

```
Y = (C, P, D, dD, V)

C  = Convex loss functional (asymmetric penalty for adverse moves)
P  = Tail mass (probability beyond threshold)
D  = Epistemic disagreement (entropy of model posterior)
dD = Disagreement momentum (rate of change)
V  = Volatility compression ratio
```

**Transition Dynamics**

State transitions follow a constrained Markov process:

```
P(S‚Çú | S‚Çú‚Çã‚ÇÅ, Y) ‚àù P(Y | S‚Çú) ¬∑ P(S‚Çú | S‚Çú‚Çã‚ÇÅ)
```

With diagonal dominance (persistence ‚âà 0.85) and forward-only transitions.

**Decision Rule**

Switch debt when:

```
P(PRE_POLICY | Y) > Œ±
```

Default Œ± = 0.60. The decision is **irreversible** (once triggered, done).

---

### Calibration: PIT Test

**Probability Integral Transform**

If predictions are well-calibrated:

```
u = F(r_actual)  should be  ~ Uniform(0, 1)
```

Where F is the predicted CDF.

**KS Test**

We compute Kolmogorov-Smirnov statistic:

```
KS = sup_u | F_empirical(u) - u |
```

p-value > 0.05 indicates calibration is acceptable.

**Interpretation**

- KS ‚âà 0: Perfect calibration
- KS > 0.1: Miscalibration detected
- Systematic U-shape in PIT histogram: Overconfidence
- Systematic ‚à©-shape: Underconfidence

---

### Summary: The Mathematical Contract

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                             ‚îÇ
‚îÇ   DATA:     r‚Çú = log(P‚Çú/P‚Çú‚Çã‚ÇÅ)                                 ‚îÇ
‚îÇ             œÉ‚Çú¬≤ = EWMA(r‚Çú¬≤)                                  ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   TUNING:   Œº‚Çú = œÜŒº‚Çú‚Çã‚ÇÅ + Œ∑‚Çú        (state equation)           ‚îÇ
‚îÇ             r‚Çú = Œº‚Çú + Œµ‚Çú           (observation)              ‚îÇ
‚îÇ             q* = argmax ‚Ñì(q)       (MLE)                    ‚îÇ
‚îÇ             p(m|r) ‚àù exp(-BIC/2)   (BMA weights)            ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   SIGNAL:   p(r|data) = Œ£‚Çò p(r|m,Œ∏) ¬∑ p(m|r)   (mixture)    ‚îÇ
‚îÇ             P(r>0) = ‚à´‚ÇÄ^‚àû p(r) dr              (probability)‚îÇ
‚îÇ             signal = map(P(r>0))               (decision)   ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

The math is the system. The code merely implements it.

---

## Cheat Sheet

### First Time Setup
```bash
make setup              # Install everything, download data
```

### Daily Use
```bash
make stocks             # The one command you need
```

### Weekly Maintenance
```bash
make tune               # Re-calibrate parameters
make stocks             # Generate fresh signals
```

### When Things Break
```bash
make doctor             # Reinstall dependencies
make failed             # See what failed
make purge              # Clear failed cache
make data               # Re-download everything
```

### Quick Reference Table

| I want to... | Command |
|--------------|---------|
| Generate signals | `make stocks` |
| Just see cached signals | `make report` |
| Re-tune all parameters | `make tune ARGS="--force"` |
| Test with few assets | `make top20` |
| Validate calibration | `make fx-calibration` |
| Clear everything | `make clear` |
| See what failed | `make failed` |
| Work offline | `OFFLINE_MODE=1 make stocks` |

---

## Troubleshooting

### Common Issues

**`zsh: command not found: python`**
```bash
echo 'alias python=python3' >> ~/.zshrc && source ~/.zshrc
```

**Build errors on Apple Silicon**
```bash
xcode-select --install
```

**Assets failing to download**
```bash
make failed    # See which assets failed
make purge     # Clear their cache
make data      # Re-download
```

**Stale parameters**
```bash
make clear-q   # Clear parameter cache
make tune      # Re-estimate
make stocks    # Fresh signals
```

### Environment Variables

| Variable | Description |
|----------|-------------|
| `PRICE_DATA_DIR` | Override data cache location |
| `NO_COLOR=1` | Disable colored output |
| `PYTHON` | Force specific interpreter |
| `OFFLINE_MODE=1` | Use cached data only, no network calls |

---

## Philosophy

### The Core Principle

> *"Act only on beliefs that were actually learned."*

This system is a **belief evolution engine**. It maintains competing hypotheses about market dynamics and lets Bayesian inference arbitrate between them.

### What Makes This Different

| Traditional Systems | This System |
|---------------------|-------------|
| Pick the "best" model | Maintain model uncertainty |
| Point estimates | Full distributions |
| Fixed parameters | Continuously re-calibrated |
| Confidence from conviction | Confidence from calibration |
| Fail silently when wrong | Know when you don't know |

### The Three Laws

1. **Never invent beliefs.** When evidence is weak, become more ignorant‚Äînot more confident. Fallback is always hierarchical (regime ‚Üí global), never fabricated.

2. **Preserve distributional integrity.** Decisions come from distributions, not point estimates. The signal layer sees samples, not parameters.

3. **Separate epistemology from agency.** The Tuning Engine learns beliefs. The Signal Engine acts on them. They never mix.

### The Goal

**Calibrated uncertainty**, not false precision.

When the system says "62% probability," it should be right 62% of the time. Not 70%. Not 55%. Exactly 62%.

That's what the PIT calibration tests verify. That's what makes this system trustworthy.

---

## License

This project is for educational and research purposes. See individual dependencies for their respective licenses.

---

<p align="center">
  <sub>Built with scientific rigor and engineering craftsmanship.</sub>
</p>

<p align="center">
  <sub>The math is the system. The code merely implements it.</sub>
</p>